# Neural Networks MCQ Questions (60 questions)
# Run with: mix run priv/repo/seeds/mcq_neural_networks.exs

alias OsnAiPrep.Repo
alias OsnAiPrep.Mcq.Question

defmodule NNSeeder do
  def insert_all(questions) do
    for q <- questions do
      %Question{}
      |> Question.changeset(q)
      |> Repo.insert!()
    end
    length(questions)
  end
end

nn_questions = [
  %{question_en: "What is a perceptron?", question_id: "Apa itu perceptron?", option_a_en: "A single-layer neural network for binary classification", option_a_id: "Neural network satu layer untuk klasifikasi biner", option_b_en: "A type of optimizer", option_b_id: "Jenis optimizer", option_c_en: "A loss function", option_c_id: "Fungsi loss", option_d_en: "A pooling layer", option_d_id: "Layer pooling", correct_answer: "A", explanation_en: "Perceptron is the simplest neural network: weighted sum of inputs through activation function.", explanation_id: "Perceptron adalah neural network paling sederhana: jumlah berbobot input melalui fungsi aktivasi.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is the vanishing gradient problem?", question_id: "Apa itu masalah vanishing gradient?", option_a_en: "Gradients become extremely small in deep networks, stopping learning", option_a_id: "Gradient menjadi sangat kecil di network dalam, menghentikan pembelajaran", option_b_en: "Gradients disappear completely", option_b_id: "Gradient menghilang sepenuhnya", option_c_en: "Weights become zero", option_c_id: "Bobot menjadi nol", option_d_en: "Loss becomes infinite", option_d_id: "Loss menjadi tak terhingga", correct_answer: "A", explanation_en: "With sigmoid/tanh, gradients shrink exponentially through layers, making early layers learn very slowly.", explanation_id: "Dengan sigmoid/tanh, gradient menyusut secara eksponensial melalui layer, membuat layer awal belajar sangat lambat.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "How does ReLU help with vanishing gradients?", question_id: "Bagaimana ReLU membantu dengan vanishing gradient?", option_a_en: "ReLU has gradient of 1 for positive inputs, preventing shrinkage", option_a_id: "ReLU memiliki gradient 1 untuk input positif, mencegah penyusutan", option_b_en: "ReLU removes gradients", option_b_id: "ReLU menghapus gradient", option_c_en: "ReLU doubles gradients", option_c_id: "ReLU menggandakan gradient", option_d_en: "ReLU averages gradients", option_d_id: "ReLU merata-ratakan gradient", correct_answer: "A", explanation_en: "ReLU(x)=max(0,x) has derivative of 1 for x>0, maintaining gradient magnitude during backprop.", explanation_id: "ReLU(x)=max(0,x) memiliki turunan 1 untuk x>0, mempertahankan magnitude gradient selama backprop.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the dying ReLU problem?", question_id: "Apa itu masalah dying ReLU?", option_a_en: "Neurons output zero for all inputs and stop learning", option_a_id: "Neuron menghasilkan nol untuk semua input dan berhenti belajar", option_b_en: "ReLU becomes too slow", option_b_id: "ReLU menjadi terlalu lambat", option_c_en: "ReLU uses too much memory", option_c_id: "ReLU menggunakan terlalu banyak memori", option_d_en: "ReLU outputs infinity", option_d_id: "ReLU menghasilkan infinity", correct_answer: "A", explanation_en: "If inputs are always negative, ReLU outputs 0 and gradient is 0, so neuron never updates.", explanation_id: "Jika input selalu negatif, ReLU menghasilkan 0 dan gradient adalah 0, jadi neuron tidak pernah update.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is Leaky ReLU?", question_id: "Apa itu Leaky ReLU?", option_a_en: "ReLU variant that allows small negative values: f(x) = x if x>0, else ax", option_a_id: "Varian ReLU yang memungkinkan nilai negatif kecil: f(x) = x jika x>0, jika tidak ax", option_b_en: "A slower version of ReLU", option_b_id: "Versi ReLU yang lebih lambat", option_c_en: "A broken ReLU", option_c_id: "ReLU yang rusak", option_d_en: "ReLU with memory leak", option_d_id: "ReLU dengan memory leak", correct_answer: "A", explanation_en: "Leaky ReLU prevents dying neurons by having small slope (typically 0.01) for negative inputs.", explanation_id: "Leaky ReLU mencegah neuron mati dengan memiliki kemiringan kecil (biasanya 0.01) untuk input negatif.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is softmax used for?", question_id: "Untuk apa softmax digunakan?", option_a_en: "Converting outputs to probability distribution for multi-class classification", option_a_id: "Mengkonversi output ke distribusi probabilitas untuk klasifikasi multi-kelas", option_b_en: "Making outputs softer", option_b_id: "Membuat output lebih lembut", option_c_en: "Reducing model size", option_c_id: "Mengurangi ukuran model", option_d_en: "Binary classification", option_d_id: "Klasifikasi biner", correct_answer: "A", explanation_en: "Softmax normalizes outputs to probabilities summing to 1: softmax(x_i) = e^x_i / sum(e^x_j).", explanation_id: "Softmax menormalisasi output ke probabilitas yang berjumlah 1: softmax(x_i) = e^x_i / sum(e^x_j).", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is batch normalization?", question_id: "Apa itu batch normalization?", option_a_en: "Normalizing layer inputs to have zero mean and unit variance", option_a_id: "Menormalisasi input layer agar memiliki mean nol dan variance unit", option_b_en: "Making batch sizes equal", option_b_id: "Membuat ukuran batch sama", option_c_en: "Normalizing batch order", option_c_id: "Menormalisasi urutan batch", option_d_en: "Reducing batch processing time", option_d_id: "Mengurangi waktu pemrosesan batch", correct_answer: "A", explanation_en: "BatchNorm stabilizes training by normalizing activations, allowing higher learning rates and faster convergence.", explanation_id: "BatchNorm menstabilkan training dengan menormalisasi aktivasi, memungkinkan learning rate lebih tinggi dan konvergensi lebih cepat.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is layer normalization?", question_id: "Apa itu layer normalization?", option_a_en: "Normalizing across features instead of across batch", option_a_id: "Menormalisasi di seluruh fitur bukan di seluruh batch", option_b_en: "Same as batch normalization", option_b_id: "Sama dengan batch normalization", option_c_en: "Normalizing layer weights", option_c_id: "Menormalisasi bobot layer", option_d_en: "Reducing layer count", option_d_id: "Mengurangi jumlah layer", correct_answer: "A", explanation_en: "LayerNorm normalizes across features per sample, useful for RNNs/Transformers where batch stats vary.", explanation_id: "LayerNorm menormalisasi di seluruh fitur per sampel, berguna untuk RNN/Transformer di mana statistik batch bervariasi.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the purpose of dropout?", question_id: "Apa tujuan dari dropout?", option_a_en: "Randomly disabling neurons during training to prevent overfitting", option_a_id: "Menonaktifkan neuron secara acak selama training untuk mencegah overfitting", option_b_en: "Removing unused layers", option_b_id: "Menghapus layer yang tidak digunakan", option_c_en: "Dropping bad gradients", option_c_id: "Membuang gradient buruk", option_d_en: "Reducing network depth", option_d_id: "Mengurangi kedalaman network", correct_answer: "A", explanation_en: "Dropout forces network to learn redundant representations, acting as ensemble of sub-networks.", explanation_id: "Dropout memaksa network untuk mempelajari representasi redundan, bertindak sebagai ensemble sub-network.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What happens to dropout during inference?", question_id: "Apa yang terjadi pada dropout selama inferensi?", option_a_en: "Dropout is disabled and outputs are scaled", option_a_id: "Dropout dinonaktifkan dan output diskalakan", option_b_en: "Dropout continues as usual", option_b_id: "Dropout berlanjut seperti biasa", option_c_en: "Dropout rate doubles", option_c_id: "Rate dropout berlipat ganda", option_d_en: "Only half the neurons work", option_d_id: "Hanya setengah neuron yang bekerja", correct_answer: "A", explanation_en: "At inference, all neurons are active but outputs are scaled by (1-p) to match training expectation.", explanation_id: "Saat inferensi, semua neuron aktif tapi output diskalakan oleh (1-p) untuk mencocokkan ekspektasi training.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is weight initialization?", question_id: "Apa itu inisialisasi bobot?", option_a_en: "Setting initial weight values before training starts", option_a_id: "Mengatur nilai bobot awal sebelum training dimulai", option_b_en: "Removing weights", option_b_id: "Menghapus bobot", option_c_en: "Final weight values", option_c_id: "Nilai bobot akhir", option_d_en: "Weight during inference", option_d_id: "Bobot selama inferensi", correct_answer: "A", explanation_en: "Proper initialization (Xavier, He) prevents vanishing/exploding gradients at the start of training.", explanation_id: "Inisialisasi yang tepat (Xavier, He) mencegah vanishing/exploding gradient di awal training.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is Xavier/Glorot initialization?", question_id: "Apa itu inisialisasi Xavier/Glorot?", option_a_en: "Initializing weights with variance 2/(fan_in + fan_out)", option_a_id: "Menginisialisasi bobot dengan variance 2/(fan_in + fan_out)", option_b_en: "Setting all weights to zero", option_b_id: "Mengatur semua bobot ke nol", option_c_en: "Using random large values", option_c_id: "Menggunakan nilai acak besar", option_d_en: "Copying weights from another model", option_d_id: "Menyalin bobot dari model lain", correct_answer: "A", explanation_en: "Xavier init keeps variance stable across layers, designed for tanh/sigmoid activations.", explanation_id: "Inisialisasi Xavier menjaga variance stabil di seluruh layer, dirancang untuk aktivasi tanh/sigmoid.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is He initialization?", question_id: "Apa itu inisialisasi He?", option_a_en: "Initializing weights with variance 2/fan_in, designed for ReLU", option_a_id: "Menginisialisasi bobot dengan variance 2/fan_in, dirancang untuk ReLU", option_b_en: "Named after a person called He", option_b_id: "Dinamai dari orang bernama He", option_c_en: "Same as Xavier", option_c_id: "Sama dengan Xavier", option_d_en: "Using ones for weights", option_d_id: "Menggunakan satu untuk bobot", correct_answer: "A", explanation_en: "He init accounts for ReLU zeroing half the outputs, using larger initial weights than Xavier.", explanation_id: "Inisialisasi He memperhitungkan ReLU yang menolkan setengah output, menggunakan bobot awal lebih besar dari Xavier.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is momentum in optimization?", question_id: "Apa itu momentum dalam optimisasi?", option_a_en: "Accumulating past gradients to accelerate learning in consistent directions", option_a_id: "Mengakumulasi gradient masa lalu untuk mempercepat pembelajaran dalam arah konsisten", option_b_en: "Physical momentum of weights", option_b_id: "Momentum fisik bobot", option_c_en: "Speed of training", option_c_id: "Kecepatan training", option_d_en: "Batch size", option_d_id: "Ukuran batch", correct_answer: "A", explanation_en: "Momentum helps escape local minima and smooths optimization by averaging gradient directions.", explanation_id: "Momentum membantu keluar dari minima lokal dan memperhalus optimisasi dengan merata-ratakan arah gradient.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the Adam optimizer?", question_id: "Apa itu optimizer Adam?", option_a_en: "Adaptive Moment Estimation combining momentum and RMSprop", option_a_id: "Adaptive Moment Estimation menggabungkan momentum dan RMSprop", option_b_en: "A person who optimizes", option_b_id: "Orang yang mengoptimasi", option_c_en: "Basic gradient descent", option_c_id: "Gradient descent dasar", option_d_en: "A type of loss function", option_d_id: "Jenis fungsi loss", correct_answer: "A", explanation_en: "Adam adapts learning rate per parameter using first and second moment estimates of gradients.", explanation_id: "Adam menyesuaikan learning rate per parameter menggunakan estimasi momen pertama dan kedua dari gradient.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is learning rate decay?", question_id: "Apa itu learning rate decay?", option_a_en: "Gradually reducing learning rate during training", option_a_id: "Mengurangi learning rate secara bertahap selama training", option_b_en: "Learning rate becoming negative", option_b_id: "Learning rate menjadi negatif", option_c_en: "Forgetting learned weights", option_c_id: "Melupakan bobot yang dipelajari", option_d_en: "Model degradation", option_d_id: "Degradasi model", correct_answer: "A", explanation_en: "Decay allows large steps initially for fast progress, then small steps for fine-tuning near optimum.", explanation_id: "Decay memungkinkan langkah besar di awal untuk kemajuan cepat, lalu langkah kecil untuk fine-tuning dekat optimum.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is early stopping?", question_id: "Apa itu early stopping?", option_a_en: "Stopping training when validation loss stops improving", option_a_id: "Menghentikan training ketika validation loss berhenti membaik", option_b_en: "Starting training late", option_b_id: "Memulai training terlambat", option_c_en: "Stopping after first epoch", option_c_id: "Berhenti setelah epoch pertama", option_d_en: "Limiting batch size", option_d_id: "Membatasi ukuran batch", correct_answer: "A", explanation_en: "Early stopping prevents overfitting by monitoring validation metrics and stopping when they plateau.", explanation_id: "Early stopping mencegah overfitting dengan memantau metrik validasi dan berhenti ketika plateau.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is an autoencoder?", question_id: "Apa itu autoencoder?", option_a_en: "A network that learns to compress and reconstruct its input", option_a_id: "Network yang belajar mengompresi dan merekonstruksi inputnya", option_b_en: "Automatic code generator", option_b_id: "Generator kode otomatis", option_c_en: "A type of classifier", option_c_id: "Jenis classifier", option_d_en: "Data encoder for files", option_d_id: "Encoder data untuk file", correct_answer: "A", explanation_en: "Autoencoders have encoder (compress) and decoder (reconstruct). Bottleneck learns useful representations.", explanation_id: "Autoencoder memiliki encoder (kompresi) dan decoder (rekonstruksi). Bottleneck mempelajari representasi yang berguna.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is a variational autoencoder (VAE)?", question_id: "Apa itu variational autoencoder (VAE)?", option_a_en: "An autoencoder that learns a probabilistic latent space", option_a_id: "Autoencoder yang mempelajari ruang laten probabilistik", option_b_en: "A variable autoencoder", option_b_id: "Autoencoder variabel", option_c_en: "Same as regular autoencoder", option_c_id: "Sama dengan autoencoder biasa", option_d_en: "An encoder with variations", option_d_id: "Encoder dengan variasi", correct_answer: "A", explanation_en: "VAE learns mean and variance of latent distribution, enabling smooth interpolation and generation.", explanation_id: "VAE mempelajari mean dan variance distribusi laten, memungkinkan interpolasi dan generasi yang halus.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is residual connection (skip connection)?", question_id: "Apa itu residual connection (skip connection)?", option_a_en: "Adding input directly to output of a layer block", option_a_id: "Menambahkan input langsung ke output dari blok layer", option_b_en: "Skipping layers randomly", option_b_id: "Melewati layer secara acak", option_c_en: "Connecting residual networks", option_c_id: "Menghubungkan residual network", option_d_en: "Removing connections", option_d_id: "Menghapus koneksi", correct_answer: "A", explanation_en: "Skip connections allow gradients to flow directly, enabling training of very deep networks (ResNet).", explanation_id: "Skip connection memungkinkan gradient mengalir langsung, memungkinkan training network sangat dalam (ResNet).", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is gradient clipping?", question_id: "Apa itu gradient clipping?", option_a_en: "Limiting gradient magnitude to prevent exploding gradients", option_a_id: "Membatasi magnitude gradient untuk mencegah exploding gradient", option_b_en: "Removing gradients", option_b_id: "Menghapus gradient", option_c_en: "Copying gradients", option_c_id: "Menyalin gradient", option_d_en: "Averaging gradients", option_d_id: "Merata-ratakan gradient", correct_answer: "A", explanation_en: "Clipping caps gradients at a threshold, preventing unstable updates in RNNs and deep networks.", explanation_id: "Clipping membatasi gradient pada threshold, mencegah update tidak stabil di RNN dan network dalam.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is L2 regularization (weight decay)?", question_id: "Apa itu regularisasi L2 (weight decay)?", option_a_en: "Adding squared weight penalty to loss function", option_a_id: "Menambahkan penalti bobot kuadrat ke fungsi loss", option_b_en: "Reducing two layers", option_b_id: "Mengurangi dua layer", option_c_en: "Using two learning rates", option_c_id: "Menggunakan dua learning rate", option_d_en: "Limiting to two classes", option_d_id: "Membatasi ke dua kelas", correct_answer: "A", explanation_en: "L2 adds lambda*sum(w^2) to loss, encouraging smaller weights and preventing overfitting.", explanation_id: "L2 menambahkan lambda*sum(w^2) ke loss, mendorong bobot lebih kecil dan mencegah overfitting.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is L1 regularization?", question_id: "Apa itu regularisasi L1?", option_a_en: "Adding absolute weight penalty, encouraging sparsity", option_a_id: "Menambahkan penalti bobot absolut, mendorong sparsity", option_b_en: "Using one layer only", option_b_id: "Menggunakan satu layer saja", option_c_en: "First-order optimization", option_c_id: "Optimisasi orde pertama", option_d_en: "Linear activation", option_d_id: "Aktivasi linear", correct_answer: "A", explanation_en: "L1 adds lambda*sum(|w|) to loss, pushing some weights to exactly zero (feature selection).", explanation_id: "L1 menambahkan lambda*sum(|w|) ke loss, mendorong beberapa bobot ke nol tepat (seleksi fitur).", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the difference between L1 and L2 regularization?", question_id: "Apa perbedaan antara regularisasi L1 dan L2?", option_a_en: "L1 produces sparse weights, L2 produces small but non-zero weights", option_a_id: "L1 menghasilkan bobot sparse, L2 menghasilkan bobot kecil tapi tidak nol", option_b_en: "They are the same", option_b_id: "Keduanya sama", option_c_en: "L2 produces zeros", option_c_id: "L2 menghasilkan nol", option_d_en: "L1 is stronger", option_d_id: "L1 lebih kuat", correct_answer: "A", explanation_en: "L1's constant gradient pushes weights to zero; L2's proportional gradient shrinks but rarely zeros weights.", explanation_id: "Gradient konstan L1 mendorong bobot ke nol; gradient proporsional L2 menyusutkan tapi jarang menolkan bobot.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is a loss function?", question_id: "Apa itu fungsi loss?", option_a_en: "A function measuring the error between predictions and targets", option_a_id: "Fungsi yang mengukur error antara prediksi dan target", option_b_en: "A function that loses data", option_b_id: "Fungsi yang kehilangan data", option_c_en: "The opposite of gain", option_c_id: "Kebalikan dari gain", option_d_en: "Network architecture", option_d_id: "Arsitektur network", correct_answer: "A", explanation_en: "Loss functions (MSE, cross-entropy) quantify prediction error, guiding optimization.", explanation_id: "Fungsi loss (MSE, cross-entropy) mengkuantifikasi error prediksi, memandu optimisasi.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is cross-entropy loss?", question_id: "Apa itu cross-entropy loss?", option_a_en: "Loss measuring difference between predicted and true probability distributions", option_a_id: "Loss yang mengukur perbedaan antara distribusi probabilitas prediksi dan sebenarnya", option_b_en: "Crossing entropy values", option_b_id: "Menyilangkan nilai entropy", option_c_en: "Same as MSE", option_c_id: "Sama dengan MSE", option_d_en: "Loss for regression", option_d_id: "Loss untuk regresi", correct_answer: "A", explanation_en: "Cross-entropy = -sum(y*log(p)) is standard for classification, penalizing confident wrong predictions heavily.", explanation_id: "Cross-entropy = -sum(y*log(p)) adalah standar untuk klasifikasi, sangat menghukum prediksi salah yang percaya diri.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is binary cross-entropy?", question_id: "Apa itu binary cross-entropy?", option_a_en: "Cross-entropy loss for two-class classification", option_a_id: "Loss cross-entropy untuk klasifikasi dua kelas", option_b_en: "Cross-entropy with binary weights", option_b_id: "Cross-entropy dengan bobot biner", option_c_en: "Two cross-entropy losses", option_c_id: "Dua loss cross-entropy", option_d_en: "Cross-entropy in binary format", option_d_id: "Cross-entropy dalam format biner", correct_answer: "A", explanation_en: "BCE = -[y*log(p) + (1-y)*log(1-p)] for binary classification with sigmoid output.", explanation_id: "BCE = -[y*log(p) + (1-y)*log(1-p)] untuk klasifikasi biner dengan output sigmoid.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is mean squared error (MSE)?", question_id: "Apa itu mean squared error (MSE)?", option_a_en: "Average of squared differences between predictions and targets", option_a_id: "Rata-rata kuadrat perbedaan antara prediksi dan target", option_b_en: "Mean of all errors", option_b_id: "Rata-rata semua error", option_c_en: "Square root of errors", option_c_id: "Akar kuadrat error", option_d_en: "Maximum squared error", option_d_id: "Error kuadrat maksimum", correct_answer: "A", explanation_en: "MSE = mean((y - y_pred)^2) is standard regression loss, sensitive to outliers.", explanation_id: "MSE = mean((y - y_pred)^2) adalah loss regresi standar, sensitif terhadap outlier.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is the chain rule in backpropagation?", question_id: "Apa itu chain rule dalam backpropagation?", option_a_en: "Computing gradients by multiplying partial derivatives through layers", option_a_id: "Menghitung gradient dengan mengalikan turunan parsial melalui layer", option_b_en: "Chaining networks together", option_b_id: "Merantai network bersama", option_c_en: "A security chain", option_c_id: "Rantai keamanan", option_d_en: "Linking data samples", option_d_id: "Menghubungkan sampel data", correct_answer: "A", explanation_en: "Chain rule: dL/dw = dL/dy * dy/dw. Backprop applies this recursively through all layers.", explanation_id: "Chain rule: dL/dw = dL/dy * dy/dw. Backprop menerapkan ini secara rekursif melalui semua layer.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is a feedforward neural network?", question_id: "Apa itu feedforward neural network?", option_a_en: "Network where information flows only in one direction (input to output)", option_a_id: "Network di mana informasi mengalir hanya dalam satu arah (input ke output)", option_b_en: "Network that feeds itself", option_b_id: "Network yang memberi makan dirinya sendiri", option_c_en: "Network with feedback loops", option_c_id: "Network dengan loop umpan balik", option_d_en: "Network for food data", option_d_id: "Network untuk data makanan", correct_answer: "A", explanation_en: "Feedforward networks have no cycles. MLPs are feedforward, while RNNs have recurrent connections.", explanation_id: "Feedforward network tidak memiliki siklus. MLP adalah feedforward, sementara RNN memiliki koneksi recurrent.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is a hidden layer?", question_id: "Apa itu hidden layer?", option_a_en: "Layers between input and output that learn intermediate representations", option_a_id: "Layer antara input dan output yang mempelajari representasi intermediate", option_b_en: "Invisible layers", option_b_id: "Layer yang tidak terlihat", option_c_en: "Layers that hide data", option_c_id: "Layer yang menyembunyikan data", option_d_en: "Secret layers", option_d_id: "Layer rahasia", correct_answer: "A", explanation_en: "Hidden layers transform inputs into increasingly abstract features that help with the final prediction.", explanation_id: "Hidden layer mentransformasi input menjadi fitur yang semakin abstrak yang membantu prediksi akhir.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What determines the depth of a neural network?", question_id: "Apa yang menentukan kedalaman neural network?", option_a_en: "The number of hidden layers", option_a_id: "Jumlah hidden layer", option_b_en: "The number of neurons", option_b_id: "Jumlah neuron", option_c_en: "The input size", option_c_id: "Ukuran input", option_d_en: "The batch size", option_d_id: "Ukuran batch", correct_answer: "A", explanation_en: "Depth refers to layer count. Deep networks have many layers; wider networks have many neurons per layer.", explanation_id: "Kedalaman mengacu pada jumlah layer. Network dalam memiliki banyak layer; network lebih lebar memiliki banyak neuron per layer.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is the universal approximation theorem?", question_id: "Apa itu teorema aproksimasi universal?", option_a_en: "A network with one hidden layer can approximate any continuous function", option_a_id: "Network dengan satu hidden layer dapat mengaproksimasi fungsi kontinu apa pun", option_b_en: "All networks are universal", option_b_id: "Semua network adalah universal", option_c_en: "Networks work everywhere", option_c_id: "Network bekerja di mana saja", option_d_en: "Approximation is universal", option_d_id: "Aproksimasi adalah universal", correct_answer: "A", explanation_en: "With enough neurons, a single hidden layer MLP can approximate any continuous function arbitrarily well.", explanation_id: "Dengan cukup neuron, MLP satu hidden layer dapat mengaproksimasi fungsi kontinu apa pun dengan baik.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is a computational graph?", question_id: "Apa itu computational graph?", option_a_en: "A graph representing the flow of computations for automatic differentiation", option_a_id: "Graf yang merepresentasikan alur komputasi untuk diferensiasi otomatis", option_b_en: "A chart of computer performance", option_b_id: "Grafik performa komputer", option_c_en: "A graph drawn by computer", option_c_id: "Graf yang digambar oleh komputer", option_d_en: "Network topology", option_d_id: "Topologi network", correct_answer: "A", explanation_en: "Frameworks like PyTorch build computational graphs to track operations and compute gradients automatically.", explanation_id: "Framework seperti PyTorch membangun computational graph untuk melacak operasi dan menghitung gradient secara otomatis.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the purpose of bias terms in neural networks?", question_id: "Apa tujuan dari term bias di neural network?", option_a_en: "To shift the activation function, allowing neurons to activate even when inputs are zero", option_a_id: "Untuk menggeser fungsi aktivasi, memungkinkan neuron aktif bahkan ketika input nol", option_b_en: "To add prejudice to the model", option_b_id: "Untuk menambahkan prasangka ke model", option_c_en: "To reduce variance", option_c_id: "Untuk mengurangi variance", option_d_en: "To increase speed", option_d_id: "Untuk meningkatkan kecepatan", correct_answer: "A", explanation_en: "Bias allows the activation function to be shifted, similar to the intercept in linear regression.", explanation_id: "Bias memungkinkan fungsi aktivasi digeser, mirip dengan intercept dalam regresi linear.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is model capacity?", question_id: "Apa itu kapasitas model?", option_a_en: "The ability of a model to fit a wide variety of functions", option_a_id: "Kemampuan model untuk menyesuaikan berbagai fungsi", option_b_en: "How much data it can store", option_b_id: "Berapa banyak data yang dapat disimpan", option_c_en: "Maximum batch size", option_c_id: "Ukuran batch maksimum", option_d_en: "Training speed", option_d_id: "Kecepatan training", correct_answer: "A", explanation_en: "Higher capacity (more parameters) can fit complex patterns but risks overfitting. Balance is key.", explanation_id: "Kapasitas lebih tinggi (lebih banyak parameter) dapat menyesuaikan pola kompleks tapi berisiko overfitting. Keseimbangan adalah kunci.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is transfer learning?", question_id: "Apa itu transfer learning?", option_a_en: "Using a pre-trained model and adapting it for a new task", option_a_id: "Menggunakan model pre-trained dan mengadaptasinya untuk tugas baru", option_b_en: "Transferring data between models", option_b_id: "Mentransfer data antar model", option_c_en: "Moving models to new computers", option_c_id: "Memindahkan model ke komputer baru", option_d_en: "Copying model architecture", option_d_id: "Menyalin arsitektur model", correct_answer: "A", explanation_en: "Transfer learning reuses learned features from large datasets, requiring less data and training for new tasks.", explanation_id: "Transfer learning menggunakan kembali fitur yang dipelajari dari dataset besar, membutuhkan lebih sedikit data dan training untuk tugas baru.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is fine-tuning?", question_id: "Apa itu fine-tuning?", option_a_en: "Training a pre-trained model on new data with a small learning rate", option_a_id: "Melatih model pre-trained pada data baru dengan learning rate kecil", option_b_en: "Tuning musical instruments", option_b_id: "Menyetel instrumen musik", option_c_en: "Making fine adjustments to hardware", option_c_id: "Melakukan penyesuaian halus pada hardware", option_d_en: "Initial training from scratch", option_d_id: "Training awal dari nol", correct_answer: "A", explanation_en: "Fine-tuning unfreezes some pre-trained layers and trains on new data to adapt the model.", explanation_id: "Fine-tuning membuka beberapa layer pre-trained dan melatih pada data baru untuk mengadaptasi model.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is feature extraction in transfer learning?", question_id: "Apa itu ekstraksi fitur dalam transfer learning?", option_a_en: "Using frozen pre-trained layers as fixed feature extractors", option_a_id: "Menggunakan layer pre-trained yang dibekukan sebagai ekstraktor fitur tetap", option_b_en: "Extracting data from files", option_b_id: "Mengekstrak data dari file", option_c_en: "Creating new features manually", option_c_id: "Membuat fitur baru secara manual", option_d_en: "Removing features", option_d_id: "Menghapus fitur", correct_answer: "A", explanation_en: "Keep pre-trained weights frozen, only train new classifier layers on top. Fast and requires less data.", explanation_id: "Pertahankan bobot pre-trained tetap beku, hanya latih layer classifier baru di atas. Cepat dan membutuhkan lebih sedikit data.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the difference between training and inference?", question_id: "Apa perbedaan antara training dan inferensi?", option_a_en: "Training learns weights from data; inference makes predictions using learned weights", option_a_id: "Training mempelajari bobot dari data; inferensi membuat prediksi menggunakan bobot yang dipelajari", option_b_en: "They are the same process", option_b_id: "Keduanya proses yang sama", option_c_en: "Training is faster", option_c_id: "Training lebih cepat", option_d_en: "Inference updates weights", option_d_id: "Inferensi memperbarui bobot", correct_answer: "A", explanation_en: "Training adjusts weights via backprop. Inference (forward pass only) uses fixed weights for predictions.", explanation_id: "Training menyesuaikan bobot via backprop. Inferensi (forward pass saja) menggunakan bobot tetap untuk prediksi.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is a mini-batch?", question_id: "Apa itu mini-batch?", option_a_en: "A subset of training data used for one gradient update", option_a_id: "Subset dari data training yang digunakan untuk satu update gradient", option_b_en: "The smallest batch possible", option_b_id: "Batch terkecil yang mungkin", option_c_en: "A small neural network", option_c_id: "Neural network kecil", option_d_en: "Compressed data", option_d_id: "Data terkompresi", correct_answer: "A", explanation_en: "Mini-batch (typically 32-256 samples) balances stable gradients of full batch with speed of SGD.", explanation_id: "Mini-batch (biasanya 32-256 sampel) menyeimbangkan gradient stabil dari full batch dengan kecepatan SGD.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is an epoch?", question_id: "Apa itu epoch?", option_a_en: "One complete pass through the entire training dataset", option_a_id: "Satu kali melewati seluruh dataset training secara lengkap", option_b_en: "A time period in history", option_b_id: "Periode waktu dalam sejarah", option_c_en: "One gradient update", option_c_id: "Satu update gradient", option_d_en: "A batch of data", option_d_id: "Satu batch data", correct_answer: "A", explanation_en: "Training typically runs for multiple epochs. More epochs = more exposure to data, but risk overfitting.", explanation_id: "Training biasanya berjalan beberapa epoch. Lebih banyak epoch = lebih banyak eksposur ke data, tapi risiko overfitting.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is stochastic gradient descent (SGD)?", question_id: "Apa itu stochastic gradient descent (SGD)?", option_a_en: "Updating weights using gradient from one or few samples at a time", option_a_id: "Memperbarui bobot menggunakan gradient dari satu atau beberapa sampel sekaligus", option_b_en: "Random weight updates", option_b_id: "Update bobot acak", option_c_en: "Gradient on all data at once", option_c_id: "Gradient pada semua data sekaligus", option_d_en: "No gradient computation", option_d_id: "Tidak ada komputasi gradient", correct_answer: "A", explanation_en: "SGD uses random sample(s) for each update. Faster than full batch, introduces helpful noise.", explanation_id: "SGD menggunakan sampel acak untuk setiap update. Lebih cepat dari full batch, memperkenalkan noise yang membantu.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the exploding gradient problem?", question_id: "Apa itu masalah exploding gradient?", option_a_en: "Gradients grow exponentially large, causing unstable training", option_a_id: "Gradient tumbuh sangat besar secara eksponensial, menyebabkan training tidak stabil", option_b_en: "Gradients disappear", option_b_id: "Gradient menghilang", option_c_en: "Model explodes", option_c_id: "Model meledak", option_d_en: "Training becomes too fast", option_d_id: "Training menjadi terlalu cepat", correct_answer: "A", explanation_en: "Large gradients cause huge weight updates, often seen in RNNs. Fixed with gradient clipping.", explanation_id: "Gradient besar menyebabkan update bobot yang besar, sering terlihat di RNN. Diperbaiki dengan gradient clipping.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is model ensembling?", question_id: "Apa itu model ensembling?", option_a_en: "Combining predictions from multiple models for better accuracy", option_a_id: "Menggabungkan prediksi dari beberapa model untuk akurasi lebih baik", option_b_en: "Making one large model", option_b_id: "Membuat satu model besar", option_c_en: "Training models in sequence", option_c_id: "Melatih model secara berurutan", option_d_en: "Selecting the best model", option_d_id: "Memilih model terbaik", correct_answer: "A", explanation_en: "Ensembling averages or votes across models, reducing variance and often improving test accuracy.", explanation_id: "Ensembling merata-ratakan atau voting di seluruh model, mengurangi variance dan sering meningkatkan akurasi test.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is data augmentation?", question_id: "Apa itu data augmentation?", option_a_en: "Creating modified versions of training data to increase dataset size", option_a_id: "Membuat versi modifikasi dari data training untuk meningkatkan ukuran dataset", option_b_en: "Adding more raw data", option_b_id: "Menambahkan lebih banyak data mentah", option_c_en: "Removing bad data", option_c_id: "Menghapus data buruk", option_d_en: "Compressing data", option_d_id: "Mengompresi data", correct_answer: "A", explanation_en: "Augmentation (rotation, flipping, cropping) creates variations that improve model generalization.", explanation_id: "Augmentasi (rotasi, membalik, cropping) membuat variasi yang meningkatkan generalisasi model.", topic: "neural_networks", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is GELU activation?", question_id: "Apa itu aktivasi GELU?", option_a_en: "Gaussian Error Linear Unit - smooth approximation of ReLU used in Transformers", option_a_id: "Gaussian Error Linear Unit - aproksimasi halus dari ReLU yang digunakan di Transformer", option_b_en: "General Linear Unit", option_b_id: "General Linear Unit", option_c_en: "Same as ReLU", option_c_id: "Sama dengan ReLU", option_d_en: "Gated ELU", option_d_id: "Gated ELU", correct_answer: "A", explanation_en: "GELU: x*Phi(x) where Phi is Gaussian CDF. Smooth, non-monotonic, used in BERT/GPT.", explanation_id: "GELU: x*Phi(x) di mana Phi adalah CDF Gaussian. Halus, non-monotonik, digunakan di BERT/GPT.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is Swish activation?", question_id: "Apa itu aktivasi Swish?", option_a_en: "x * sigmoid(x) - a smooth self-gated activation", option_a_id: "x * sigmoid(x) - aktivasi self-gated yang halus", option_b_en: "A swimming activation", option_b_id: "Aktivasi berenang", option_c_en: "Same as tanh", option_c_id: "Sama dengan tanh", option_d_en: "Swiss activation", option_d_id: "Aktivasi Swiss", correct_answer: "A", explanation_en: "Swish is smooth, non-monotonic, and often outperforms ReLU in deep networks.", explanation_id: "Swish halus, non-monotonik, dan sering mengungguli ReLU di network dalam.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is ELU activation?", question_id: "Apa itu aktivasi ELU?", option_a_en: "Exponential Linear Unit - like ReLU but smooth and allows negative values", option_a_id: "Exponential Linear Unit - seperti ReLU tapi halus dan memungkinkan nilai negatif", option_b_en: "Extended Linear Unit", option_b_id: "Extended Linear Unit", option_c_en: "Same as sigmoid", option_c_id: "Sama dengan sigmoid", option_d_en: "Elastic Linear Unit", option_d_id: "Elastic Linear Unit", correct_answer: "A", explanation_en: "ELU: x if x>0, else alpha*(exp(x)-1). Pushes mean activations closer to zero.", explanation_id: "ELU: x jika x>0, jika tidak alpha*(exp(x)-1). Mendorong mean aktivasi lebih dekat ke nol.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the Huber loss?", question_id: "Apa itu loss Huber?", option_a_en: "A loss that is quadratic for small errors and linear for large errors", option_a_id: "Loss yang kuadratik untuk error kecil dan linear untuk error besar", option_b_en: "Named after a person", option_b_id: "Dinamai dari seseorang", option_c_en: "Same as MSE", option_c_id: "Sama dengan MSE", option_d_en: "A classification loss", option_d_id: "Loss klasifikasi", correct_answer: "A", explanation_en: "Huber loss combines MSE (small errors) and MAE (large errors), being robust to outliers.", explanation_id: "Loss Huber menggabungkan MSE (error kecil) dan MAE (error besar), tahan terhadap outlier.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is label smoothing?", question_id: "Apa itu label smoothing?", option_a_en: "Replacing hard labels (0/1) with soft labels to reduce overconfidence", option_a_id: "Mengganti label keras (0/1) dengan label lembut untuk mengurangi overconfidence", option_b_en: "Smoothing the data", option_b_id: "Menghaluskan data", option_c_en: "Averaging labels", option_c_id: "Merata-ratakan label", option_d_en: "Removing labels", option_d_id: "Menghapus label", correct_answer: "A", explanation_en: "Instead of [0,0,1,0], use [0.025,0.025,0.925,0.025] to prevent overconfident predictions.", explanation_id: "Alih-alih [0,0,1,0], gunakan [0.025,0.025,0.925,0.025] untuk mencegah prediksi terlalu percaya diri.", topic: "neural_networks", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is knowledge distillation?", question_id: "Apa itu knowledge distillation?", option_a_en: "Training a smaller model to mimic a larger model's outputs", option_a_id: "Melatih model lebih kecil untuk meniru output model lebih besar", option_b_en: "Extracting water from models", option_b_id: "Mengekstrak air dari model", option_c_en: "Removing knowledge", option_c_id: "Menghapus pengetahuan", option_d_en: "Making models smarter", option_d_id: "Membuat model lebih pintar", correct_answer: "A", explanation_en: "Student model learns from teacher's soft predictions, capturing dark knowledge about class relationships.", explanation_id: "Model student belajar dari prediksi lembut teacher, menangkap dark knowledge tentang hubungan kelas.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is quantization in neural networks?", question_id: "Apa itu kuantisasi di neural network?", option_a_en: "Reducing precision of weights (e.g., float32 to int8) for faster inference", option_a_id: "Mengurangi presisi bobot (mis., float32 ke int8) untuk inferensi lebih cepat", option_b_en: "Counting neurons", option_b_id: "Menghitung neuron", option_c_en: "Adding more precision", option_c_id: "Menambahkan lebih banyak presisi", option_d_en: "Quantum computing", option_d_id: "Komputasi quantum", correct_answer: "A", explanation_en: "Quantization reduces model size and speeds up inference on devices with limited resources.", explanation_id: "Kuantisasi mengurangi ukuran model dan mempercepat inferensi pada perangkat dengan sumber daya terbatas.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is pruning in neural networks?", question_id: "Apa itu pruning di neural network?", option_a_en: "Removing unimportant weights or neurons to reduce model size", option_a_id: "Menghapus bobot atau neuron yang tidak penting untuk mengurangi ukuran model", option_b_en: "Growing the network", option_b_id: "Menumbuhkan network", option_c_en: "Adding more layers", option_c_id: "Menambahkan lebih banyak layer", option_d_en: "Training faster", option_d_id: "Training lebih cepat", correct_answer: "A", explanation_en: "Pruning removes weights close to zero, creating sparse networks that are smaller and faster.", explanation_id: "Pruning menghapus bobot yang mendekati nol, membuat network sparse yang lebih kecil dan lebih cepat.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is mixed precision training?", question_id: "Apa itu mixed precision training?", option_a_en: "Using both FP16 and FP32 to speed up training while maintaining accuracy", option_a_id: "Menggunakan FP16 dan FP32 untuk mempercepat training sambil mempertahankan akurasi", option_b_en: "Mixing different optimizers", option_b_id: "Mencampur optimizer berbeda", option_c_en: "Training with mixed data", option_c_id: "Training dengan data campuran", option_d_en: "Using different batch sizes", option_d_id: "Menggunakan ukuran batch berbeda", correct_answer: "A", explanation_en: "FP16 for forward/backward pass (fast), FP32 for weight updates (accurate). Common on GPUs.", explanation_id: "FP16 untuk forward/backward pass (cepat), FP32 untuk update bobot (akurat). Umum di GPU.", topic: "neural_networks", difficulty: "hard", competition: "noai_prelim"}
]

IO.puts("Inserting Neural Networks questions...")
count = NNSeeder.insert_all(nn_questions)
IO.puts("Inserted #{count} Neural Networks questions")

total = Repo.aggregate(Question, :count)
IO.puts("\nTotal MCQ questions now: #{total}")
