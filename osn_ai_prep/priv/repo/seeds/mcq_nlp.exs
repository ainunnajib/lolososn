# NLP MCQ Questions (60 questions)
# Run with: mix run priv/repo/seeds/mcq_nlp.exs

alias OsnAiPrep.Repo
alias OsnAiPrep.Mcq.Question

defmodule NLPSeeder do
  def insert_all(questions) do
    for q <- questions do
      %Question{}
      |> Question.changeset(q)
      |> Repo.insert!()
    end
    length(questions)
  end
end

nlp_questions = [
  %{question_en: "What is tokenization?", question_id: "Apa itu tokenisasi?", option_a_en: "Breaking text into smaller units like words or subwords", option_a_id: "Memecah teks menjadi unit lebih kecil seperti kata atau subkata", option_b_en: "Creating tokens for authentication", option_b_id: "Membuat token untuk autentikasi", option_c_en: "Encrypting text", option_c_id: "Mengenkripsi teks", option_d_en: "Compressing text", option_d_id: "Mengompresi teks", correct_answer: "A", explanation_en: "Tokenization splits text into processable units. Modern models use subword tokenizers like BPE.", explanation_id: "Tokenisasi memecah teks menjadi unit yang dapat diproses. Model modern menggunakan tokenizer subkata seperti BPE.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is word embedding?", question_id: "Apa itu word embedding?", option_a_en: "Dense vector representation of words capturing semantic meaning", option_a_id: "Representasi vektor padat dari kata yang menangkap makna semantik", option_b_en: "Embedding words in documents", option_b_id: "Menyematkan kata dalam dokumen", option_c_en: "Word compression", option_c_id: "Kompresi kata", option_d_en: "Word encryption", option_d_id: "Enkripsi kata", correct_answer: "A", explanation_en: "Embeddings map words to vectors where similar words have similar vectors. Word2Vec, GloVe are classic.", explanation_id: "Embedding memetakan kata ke vektor di mana kata serupa memiliki vektor serupa. Word2Vec, GloVe adalah klasik.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is Word2Vec?", question_id: "Apa itu Word2Vec?", option_a_en: "A neural network method to learn word embeddings from context", option_a_id: "Metode neural network untuk mempelajari embedding kata dari konteks", option_b_en: "Converting words to vectors manually", option_b_id: "Mengkonversi kata ke vektor secara manual", option_c_en: "Two-word translation", option_c_id: "Terjemahan dua kata", option_d_en: "Word counting algorithm", option_d_id: "Algoritma penghitungan kata", correct_answer: "A", explanation_en: "Word2Vec uses Skip-gram or CBOW to learn embeddings where similar contexts yield similar vectors.", explanation_id: "Word2Vec menggunakan Skip-gram atau CBOW untuk mempelajari embedding di mana konteks serupa menghasilkan vektor serupa.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the difference between Skip-gram and CBOW?", question_id: "Apa perbedaan antara Skip-gram dan CBOW?", option_a_en: "Skip-gram predicts context from word; CBOW predicts word from context", option_a_id: "Skip-gram memprediksi konteks dari kata; CBOW memprediksi kata dari konteks", option_b_en: "They are identical", option_b_id: "Keduanya identik", option_c_en: "CBOW predicts context", option_c_id: "CBOW memprediksi konteks", option_d_en: "Skip-gram uses more data", option_d_id: "Skip-gram menggunakan lebih banyak data", correct_answer: "A", explanation_en: "Skip-gram works better for rare words; CBOW is faster and works better for frequent words.", explanation_id: "Skip-gram bekerja lebih baik untuk kata jarang; CBOW lebih cepat dan bekerja lebih baik untuk kata sering.", topic: "nlp", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is GloVe?", question_id: "Apa itu GloVe?", option_a_en: "Global Vectors - embeddings learned from word co-occurrence statistics", option_a_id: "Global Vectors - embedding yang dipelajari dari statistik ko-occurrence kata", option_b_en: "A type of glove", option_b_id: "Jenis sarung tangan", option_c_en: "Global vocabulary", option_c_id: "Kosakata global", option_d_en: "Deep learning model", option_d_id: "Model deep learning", correct_answer: "A", explanation_en: "GloVe combines global co-occurrence matrix factorization with local context window methods.", explanation_id: "GloVe menggabungkan faktorisasi matriks ko-occurrence global dengan metode jendela konteks lokal.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is a transformer?", question_id: "Apa itu transformer?", option_a_en: "A neural architecture using self-attention instead of recurrence", option_a_id: "Arsitektur neural menggunakan self-attention alih-alih recurrence", option_b_en: "Electrical transformer", option_b_id: "Transformator listrik", option_c_en: "Data transformer", option_c_id: "Transformer data", option_d_en: "A type of RNN", option_d_id: "Jenis RNN", correct_answer: "A", explanation_en: "Transformers process sequences in parallel using attention, enabling faster training than RNNs.", explanation_id: "Transformer memproses sequence secara paralel menggunakan attention, memungkinkan training lebih cepat dari RNN.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is self-attention?", question_id: "Apa itu self-attention?", option_a_en: "A mechanism allowing each position to attend to all positions in a sequence", option_a_id: "Mekanisme yang memungkinkan setiap posisi memperhatikan semua posisi dalam sequence", option_b_en: "Paying attention to oneself", option_b_id: "Memperhatikan diri sendiri", option_c_en: "Automatic attention", option_c_id: "Perhatian otomatis", option_d_en: "Single attention layer", option_d_id: "Layer attention tunggal", correct_answer: "A", explanation_en: "Self-attention computes weighted sum of all positions based on learned query-key-value projections.", explanation_id: "Self-attention menghitung jumlah berbobot dari semua posisi berdasarkan proyeksi query-key-value yang dipelajari.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What are Query, Key, and Value in attention?", question_id: "Apa itu Query, Key, dan Value dalam attention?", option_a_en: "Learned projections: Query asks, Key matches, Value provides the content", option_a_id: "Proyeksi yang dipelajari: Query bertanya, Key mencocokkan, Value menyediakan konten", option_b_en: "Database terms", option_b_id: "Istilah database", option_c_en: "Input, output, weight", option_c_id: "Input, output, bobot", option_d_en: "Start, middle, end", option_d_id: "Awal, tengah, akhir", correct_answer: "A", explanation_en: "Attention(Q,K,V) = softmax(QK^T/sqrt(d))V. Q determines what to look for, K what to match, V what to retrieve.", explanation_id: "Attention(Q,K,V) = softmax(QK^T/sqrt(d))V. Q menentukan apa yang dicari, K apa yang dicocokkan, V apa yang diambil.", topic: "nlp", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is multi-head attention?", question_id: "Apa itu multi-head attention?", option_a_en: "Running multiple attention operations in parallel with different projections", option_a_id: "Menjalankan beberapa operasi attention secara paralel dengan proyeksi berbeda", option_b_en: "Attention with many heads", option_b_id: "Attention dengan banyak kepala", option_c_en: "Multiple attention layers", option_c_id: "Beberapa layer attention", option_d_en: "Head-to-head comparison", option_d_id: "Perbandingan head-to-head", correct_answer: "A", explanation_en: "Multiple heads capture different types of relationships. Outputs are concatenated and projected.", explanation_id: "Beberapa head menangkap jenis hubungan yang berbeda. Output dikonkatenasi dan diproyeksikan.", topic: "nlp", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is positional encoding?", question_id: "Apa itu positional encoding?", option_a_en: "Adding position information to tokens since attention is order-invariant", option_a_id: "Menambahkan informasi posisi ke token karena attention bersifat order-invariant", option_b_en: "Encoding positions in code", option_b_id: "Mengkodekan posisi dalam kode", option_c_en: "Position of the encoder", option_c_id: "Posisi encoder", option_d_en: "Numbering words", option_d_id: "Menomori kata", correct_answer: "A", explanation_en: "Transformers add sinusoidal or learned position embeddings so the model knows word order.", explanation_id: "Transformer menambahkan embedding posisi sinusoidal atau yang dipelajari agar model mengetahui urutan kata.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is BERT?", question_id: "Apa itu BERT?", option_a_en: "Bidirectional Encoder Representations from Transformers - a pre-trained language model", option_a_id: "Bidirectional Encoder Representations from Transformers - model bahasa pre-trained", option_b_en: "A person's name", option_b_id: "Nama seseorang", option_c_en: "A type of bird", option_c_id: "Jenis burung", option_d_en: "Basic Encoder for Text", option_d_id: "Basic Encoder untuk Teks", correct_answer: "A", explanation_en: "BERT uses masked language modeling to learn bidirectional context, great for understanding tasks.", explanation_id: "BERT menggunakan masked language modeling untuk mempelajari konteks bidirectional, bagus untuk tugas pemahaman.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is masked language modeling (MLM)?", question_id: "Apa itu masked language modeling (MLM)?", option_a_en: "Pre-training by predicting randomly masked tokens from context", option_a_id: "Pre-training dengan memprediksi token yang di-mask secara acak dari konteks", option_b_en: "Hiding the model", option_b_id: "Menyembunyikan model", option_c_en: "Masking output", option_c_id: "Meng-mask output", option_d_en: "Language masking", option_d_id: "Penyamaran bahasa", correct_answer: "A", explanation_en: "MLM masks 15% of tokens and trains model to predict them, enabling bidirectional learning.", explanation_id: "MLM meng-mask 15% token dan melatih model untuk memprediksinya, memungkinkan pembelajaran bidirectional.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is GPT?", question_id: "Apa itu GPT?", option_a_en: "Generative Pre-trained Transformer - autoregressive language model for generation", option_a_id: "Generative Pre-trained Transformer - model bahasa autoregressive untuk generasi", option_b_en: "General Purpose Technology", option_b_id: "General Purpose Technology", option_c_en: "A chat application", option_c_id: "Aplikasi chat", option_d_en: "Graph Processing Tool", option_d_id: "Graph Processing Tool", correct_answer: "A", explanation_en: "GPT predicts next token given previous context. Excels at generation, completion, and in-context learning.", explanation_id: "GPT memprediksi token berikutnya berdasarkan konteks sebelumnya. Unggul dalam generasi, penyelesaian, dan in-context learning.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the difference between BERT and GPT?", question_id: "Apa perbedaan antara BERT dan GPT?", option_a_en: "BERT is bidirectional encoder; GPT is autoregressive decoder", option_a_id: "BERT adalah encoder bidirectional; GPT adalah decoder autoregressive", option_b_en: "They are the same", option_b_id: "Keduanya sama", option_c_en: "GPT is bidirectional", option_c_id: "GPT adalah bidirectional", option_d_en: "BERT generates text", option_d_id: "BERT menghasilkan teks", correct_answer: "A", explanation_en: "BERT sees all tokens (understanding). GPT sees only past tokens (generation). Different pre-training objectives.", explanation_id: "BERT melihat semua token (pemahaman). GPT melihat hanya token sebelumnya (generasi). Tujuan pre-training berbeda.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is fine-tuning in NLP?", question_id: "Apa itu fine-tuning dalam NLP?", option_a_en: "Adapting a pre-trained model to a specific task with task-specific data", option_a_id: "Mengadaptasi model pre-trained ke tugas spesifik dengan data tugas-spesifik", option_b_en: "Initial training", option_b_id: "Training awal", option_c_en: "Making the model smaller", option_c_id: "Membuat model lebih kecil", option_d_en: "Tuning hyperparameters", option_d_id: "Menyetel hyperparameter", correct_answer: "A", explanation_en: "Fine-tuning adds task-specific head and trains on labeled data with lower learning rate.", explanation_id: "Fine-tuning menambahkan head tugas-spesifik dan melatih pada data berlabel dengan learning rate lebih rendah.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is text classification?", question_id: "Apa itu klasifikasi teks?", option_a_en: "Assigning predefined categories to text documents", option_a_id: "Menetapkan kategori yang telah ditentukan ke dokumen teks", option_b_en: "Sorting text alphabetically", option_b_id: "Mengurutkan teks berdasarkan abjad", option_c_en: "Generating text", option_c_id: "Menghasilkan teks", option_d_en: "Translating text", option_d_id: "Menerjemahkan teks", correct_answer: "A", explanation_en: "Classification assigns labels like sentiment (positive/negative), topic, or spam/not spam.", explanation_id: "Klasifikasi menetapkan label seperti sentimen (positif/negatif), topik, atau spam/bukan spam.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is sentiment analysis?", question_id: "Apa itu analisis sentimen?", option_a_en: "Determining the emotional tone or opinion in text", option_a_id: "Menentukan nada emosional atau opini dalam teks", option_b_en: "Counting sentences", option_b_id: "Menghitung kalimat", option_c_en: "Grammar checking", option_c_id: "Pemeriksaan tata bahasa", option_d_en: "Text summarization", option_d_id: "Ringkasan teks", correct_answer: "A", explanation_en: "Sentiment analysis classifies text as positive, negative, or neutral. Used for reviews, social media.", explanation_id: "Analisis sentimen mengklasifikasi teks sebagai positif, negatif, atau netral. Digunakan untuk ulasan, media sosial.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is named entity recognition (NER)?", question_id: "Apa itu named entity recognition (NER)?", option_a_en: "Identifying and classifying named entities like persons, organizations, locations", option_a_id: "Mengidentifikasi dan mengklasifikasi entitas bernama seperti orang, organisasi, lokasi", option_b_en: "Naming entities", option_b_id: "Memberi nama entitas", option_c_en: "Creating names", option_c_id: "Membuat nama", option_d_en: "Entity generation", option_d_id: "Generasi entitas", correct_answer: "A", explanation_en: "NER extracts structured information: 'Apple' (ORG), 'Tim Cook' (PERSON), 'California' (LOC).", explanation_id: "NER mengekstrak informasi terstruktur: 'Apple' (ORG), 'Tim Cook' (PERSON), 'California' (LOC).", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is question answering?", question_id: "Apa itu question answering?", option_a_en: "Extracting or generating answers to questions from given context", option_a_id: "Mengekstrak atau menghasilkan jawaban untuk pertanyaan dari konteks yang diberikan", option_b_en: "Asking questions", option_b_id: "Mengajukan pertanyaan", option_c_en: "Creating questions", option_c_id: "Membuat pertanyaan", option_d_en: "Answering surveys", option_d_id: "Menjawab survei", correct_answer: "A", explanation_en: "QA systems extract answer spans from passages (extractive) or generate answers (abstractive).", explanation_id: "Sistem QA mengekstrak span jawaban dari passage (ekstraktif) atau menghasilkan jawaban (abstraktif).", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is machine translation?", question_id: "Apa itu machine translation?", option_a_en: "Automatically translating text from one language to another", option_a_id: "Menerjemahkan teks secara otomatis dari satu bahasa ke bahasa lain", option_b_en: "Machine language programming", option_b_id: "Pemrograman bahasa mesin", option_c_en: "Translating machine code", option_c_id: "Menerjemahkan kode mesin", option_d_en: "Converting data formats", option_d_id: "Mengkonversi format data", correct_answer: "A", explanation_en: "Neural MT uses encoder-decoder transformers. Google Translate, DeepL are examples.", explanation_id: "Neural MT menggunakan transformer encoder-decoder. Google Translate, DeepL adalah contoh.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is text summarization?", question_id: "Apa itu ringkasan teks?", option_a_en: "Creating a shorter version of text while preserving key information", option_a_id: "Membuat versi teks yang lebih pendek sambil mempertahankan informasi kunci", option_b_en: "Counting words", option_b_id: "Menghitung kata", option_c_en: "Expanding text", option_c_id: "Memperluas teks", option_d_en: "Correcting grammar", option_d_id: "Memperbaiki tata bahasa", correct_answer: "A", explanation_en: "Extractive selects important sentences. Abstractive generates new summary text.", explanation_id: "Ekstraktif memilih kalimat penting. Abstraktif menghasilkan teks ringkasan baru.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is a language model?", question_id: "Apa itu model bahasa?", option_a_en: "A model that predicts probability of sequences of words", option_a_id: "Model yang memprediksi probabilitas urutan kata", option_b_en: "A model of grammar rules", option_b_id: "Model aturan tata bahasa", option_c_en: "A translation model", option_c_id: "Model terjemahan", option_d_en: "A classification model", option_d_id: "Model klasifikasi", correct_answer: "A", explanation_en: "LMs estimate P(word|context), enabling text generation, completion, and understanding.", explanation_id: "LM memperkirakan P(kata|konteks), memungkinkan generasi teks, penyelesaian, dan pemahaman.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is perplexity?", question_id: "Apa itu perplexity?", option_a_en: "A measure of how well a language model predicts text (lower is better)", option_a_id: "Ukuran seberapa baik model bahasa memprediksi teks (lebih rendah lebih baik)", option_b_en: "Confusion in the model", option_b_id: "Kebingungan dalam model", option_c_en: "Model complexity", option_c_id: "Kompleksitas model", option_d_en: "Training difficulty", option_d_id: "Kesulitan training", correct_answer: "A", explanation_en: "Perplexity = exp(cross-entropy). A model with perplexity 10 is as uncertain as choosing from 10 equally likely words.", explanation_id: "Perplexity = exp(cross-entropy). Model dengan perplexity 10 seuncertain memilih dari 10 kata yang sama kemungkinannya.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is BPE (Byte Pair Encoding)?", question_id: "Apa itu BPE (Byte Pair Encoding)?", option_a_en: "A subword tokenization algorithm that merges frequent character pairs", option_a_id: "Algoritma tokenisasi subkata yang menggabungkan pasangan karakter yang sering", option_b_en: "Byte encoding", option_b_id: "Encoding byte", option_c_en: "Pair programming", option_c_id: "Pair programming", option_d_en: "Binary encoding", option_d_id: "Encoding biner", correct_answer: "A", explanation_en: "BPE iteratively merges most frequent pairs. Handles rare words by breaking into known subwords.", explanation_id: "BPE secara iteratif menggabungkan pasangan paling sering. Menangani kata jarang dengan memecah menjadi subkata yang dikenal.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is attention mask?", question_id: "Apa itu attention mask?", option_a_en: "A binary tensor indicating which positions should be attended to", option_a_id: "Tensor biner yang menunjukkan posisi mana yang harus diperhatikan", option_b_en: "A face mask", option_b_id: "Masker wajah", option_c_en: "Hiding attention", option_c_id: "Menyembunyikan attention", option_d_en: "Mask for MLM", option_d_id: "Mask untuk MLM", correct_answer: "A", explanation_en: "Attention mask prevents attending to padding tokens (0) and allows attending to real tokens (1).", explanation_id: "Attention mask mencegah perhatian ke token padding (0) dan memungkinkan perhatian ke token asli (1).", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is causal/autoregressive attention?", question_id: "Apa itu causal/autoregressive attention?", option_a_en: "Attention that only looks at previous positions, not future ones", option_a_id: "Attention yang hanya melihat posisi sebelumnya, bukan yang akan datang", option_b_en: "Attention that causes effects", option_b_id: "Attention yang menyebabkan efek", option_c_en: "Automatic attention", option_c_id: "Attention otomatis", option_d_en: "Backward attention", option_d_id: "Attention mundur", correct_answer: "A", explanation_en: "GPT uses causal masking: position i can only attend to positions 0 to i-1, enabling generation.", explanation_id: "GPT menggunakan causal masking: posisi i hanya dapat memperhatikan posisi 0 hingga i-1, memungkinkan generasi.", topic: "nlp", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is sequence-to-sequence (seq2seq)?", question_id: "Apa itu sequence-to-sequence (seq2seq)?", option_a_en: "Architecture that transforms one sequence to another (translation, summarization)", option_a_id: "Arsitektur yang mentransformasi satu sequence ke yang lain (terjemahan, ringkasan)", option_b_en: "Sequence copying", option_b_id: "Menyalin sequence", option_c_en: "Sequential learning", option_c_id: "Pembelajaran berurutan", option_d_en: "Two sequences", option_d_id: "Dua sequence", correct_answer: "A", explanation_en: "Encoder processes input sequence, decoder generates output sequence. T5, BART are seq2seq transformers.", explanation_id: "Encoder memproses sequence input, decoder menghasilkan sequence output. T5, BART adalah transformer seq2seq.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is beam search?", question_id: "Apa itu beam search?", option_a_en: "Decoding strategy that keeps top-k partial sequences at each step", option_a_id: "Strategi decoding yang menyimpan top-k sequence parsial di setiap langkah", option_b_en: "Searching with flashlight", option_b_id: "Mencari dengan senter", option_c_en: "Light-based search", option_c_id: "Pencarian berbasis cahaya", option_d_en: "Random search", option_d_id: "Pencarian acak", correct_answer: "A", explanation_en: "Beam search expands multiple hypotheses, often producing better results than greedy decoding.", explanation_id: "Beam search memperluas beberapa hipotesis, sering menghasilkan hasil lebih baik daripada greedy decoding.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is temperature in text generation?", question_id: "Apa itu temperature dalam generasi teks?", option_a_en: "A parameter controlling randomness of predictions (lower = more deterministic)", option_a_id: "Parameter yang mengontrol keacakan prediksi (lebih rendah = lebih deterministik)", option_b_en: "Physical temperature", option_b_id: "Suhu fisik", option_c_en: "GPU temperature", option_c_id: "Suhu GPU", option_d_en: "Training speed", option_d_id: "Kecepatan training", correct_answer: "A", explanation_en: "Temperature scales logits before softmax. T=0.1 is focused, T=1.0 is diverse, T>1 is chaotic.", explanation_id: "Temperature menskalakan logit sebelum softmax. T=0.1 terfokus, T=1.0 beragam, T>1 kacau.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is top-k sampling?", question_id: "Apa itu top-k sampling?", option_a_en: "Sampling only from the k most likely next tokens", option_a_id: "Sampling hanya dari k token berikutnya yang paling mungkin", option_b_en: "Selecting top students", option_b_id: "Memilih siswa teratas", option_c_en: "K-means clustering", option_c_id: "K-means clustering", option_d_en: "K-nearest neighbors", option_d_id: "K-nearest neighbors", correct_answer: "A", explanation_en: "Top-k restricts sampling to k highest probability tokens, reducing chance of nonsensical choices.", explanation_id: "Top-k membatasi sampling ke k token dengan probabilitas tertinggi, mengurangi kemungkinan pilihan tidak masuk akal.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is top-p (nucleus) sampling?", question_id: "Apa itu top-p (nucleus) sampling?", option_a_en: "Sampling from tokens whose cumulative probability exceeds p", option_a_id: "Sampling dari token yang probabilitas kumulatifnya melebihi p", option_b_en: "Top priority sampling", option_b_id: "Sampling prioritas teratas", option_c_en: "Probability sampling", option_c_id: "Sampling probabilitas", option_d_en: "Nuclear physics", option_d_id: "Fisika nuklir", correct_answer: "A", explanation_en: "Nucleus sampling dynamically adjusts number of candidates based on probability mass, more adaptive than top-k.", explanation_id: "Nucleus sampling menyesuaikan jumlah kandidat secara dinamis berdasarkan massa probabilitas, lebih adaptif dari top-k.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is an RNN?", question_id: "Apa itu RNN?", option_a_en: "Recurrent Neural Network - processes sequences by maintaining hidden state", option_a_id: "Recurrent Neural Network - memproses sequence dengan mempertahankan hidden state", option_b_en: "Random Neural Network", option_b_id: "Random Neural Network", option_c_en: "Regular Neural Network", option_c_id: "Regular Neural Network", option_d_en: "Rotating Neural Network", option_d_id: "Rotating Neural Network", correct_answer: "A", explanation_en: "RNNs process one token at a time, updating hidden state. Good for sequences but hard to parallelize.", explanation_id: "RNN memproses satu token sekaligus, memperbarui hidden state. Bagus untuk sequence tapi sulit diparalelkan.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is LSTM?", question_id: "Apa itu LSTM?", option_a_en: "Long Short-Term Memory - RNN variant with gates to handle long-range dependencies", option_a_id: "Long Short-Term Memory - varian RNN dengan gate untuk menangani dependensi jarak jauh", option_b_en: "Last Step Training Method", option_b_id: "Last Step Training Method", option_c_en: "Large Scale Text Model", option_c_id: "Large Scale Text Model", option_d_en: "Linear State Transform Model", option_d_id: "Linear State Transform Model", correct_answer: "A", explanation_en: "LSTM uses forget, input, output gates to control information flow, solving vanishing gradient problem.", explanation_id: "LSTM menggunakan forget, input, output gate untuk mengontrol aliran informasi, menyelesaikan masalah vanishing gradient.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is GRU?", question_id: "Apa itu GRU?", option_a_en: "Gated Recurrent Unit - simplified LSTM with fewer gates", option_a_id: "Gated Recurrent Unit - LSTM yang disederhanakan dengan lebih sedikit gate", option_b_en: "General Recurrent Unit", option_b_id: "General Recurrent Unit", option_c_en: "Graph Recurrent Unit", option_c_id: "Graph Recurrent Unit", option_d_en: "Gradient Recurrent Unit", option_d_id: "Gradient Recurrent Unit", correct_answer: "A", explanation_en: "GRU combines forget and input gates into update gate, simpler than LSTM with similar performance.", explanation_id: "GRU menggabungkan forget dan input gate menjadi update gate, lebih sederhana dari LSTM dengan performa serupa.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the encoder-decoder architecture?", question_id: "Apa itu arsitektur encoder-decoder?", option_a_en: "Encoder compresses input; decoder generates output from encoding", option_a_id: "Encoder mengompresi input; decoder menghasilkan output dari encoding", option_b_en: "Two separate models", option_b_id: "Dua model terpisah", option_c_en: "Encoding and decoding text", option_c_id: "Mengkodekan dan mendekode teks", option_d_en: "Input-output model", option_d_id: "Model input-output", correct_answer: "A", explanation_en: "Encoder creates representation; decoder uses it with cross-attention to generate output sequence.", explanation_id: "Encoder membuat representasi; decoder menggunakannya dengan cross-attention untuk menghasilkan sequence output.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is cross-attention?", question_id: "Apa itu cross-attention?", option_a_en: "Attention where queries come from decoder and keys/values from encoder", option_a_id: "Attention di mana query berasal dari decoder dan key/value dari encoder", option_b_en: "Crossing attention layers", option_b_id: "Menyilangkan layer attention", option_c_en: "Angry attention", option_c_id: "Attention marah", option_d_en: "Two-way attention", option_d_id: "Attention dua arah", correct_answer: "A", explanation_en: "In seq2seq, decoder attends to encoder outputs to incorporate source information during generation.", explanation_id: "Dalam seq2seq, decoder memperhatikan output encoder untuk memasukkan informasi sumber selama generasi.", topic: "nlp", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is in-context learning?", question_id: "Apa itu in-context learning?", option_a_en: "Learning from examples provided in the prompt without updating weights", option_a_id: "Belajar dari contoh yang disediakan dalam prompt tanpa memperbarui bobot", option_b_en: "Learning in context managers", option_b_id: "Belajar dalam context manager", option_c_en: "Contextual training", option_c_id: "Training kontekstual", option_d_en: "Updating model weights", option_d_id: "Memperbarui bobot model", correct_answer: "A", explanation_en: "GPT-3+ can perform tasks given few-shot examples in the prompt, no gradient updates needed.", explanation_id: "GPT-3+ dapat melakukan tugas dengan contoh few-shot dalam prompt, tidak perlu update gradient.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is few-shot learning in LLMs?", question_id: "Apa itu few-shot learning dalam LLM?", option_a_en: "Providing a few examples in the prompt to guide the model", option_a_id: "Menyediakan beberapa contoh dalam prompt untuk memandu model", option_b_en: "Training with few data", option_b_id: "Training dengan sedikit data", option_c_en: "Taking few shots of photos", option_c_id: "Mengambil beberapa foto", option_d_en: "Quick learning", option_d_id: "Pembelajaran cepat", correct_answer: "A", explanation_en: "Few-shot prompting shows 2-5 examples of input→output format, helping LLM understand the task.", explanation_id: "Few-shot prompting menunjukkan 2-5 contoh format input→output, membantu LLM memahami tugas.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is zero-shot learning?", question_id: "Apa itu zero-shot learning?", option_a_en: "Performing tasks without any examples, just instructions", option_a_id: "Melakukan tugas tanpa contoh apa pun, hanya instruksi", option_b_en: "Learning nothing", option_b_id: "Tidak belajar apa-apa", option_c_en: "Zero training data", option_c_id: "Nol data training", option_d_en: "Immediate learning", option_d_id: "Pembelajaran langsung", correct_answer: "A", explanation_en: "Zero-shot uses only task description: 'Classify sentiment:' without showing labeled examples.", explanation_id: "Zero-shot hanya menggunakan deskripsi tugas: 'Klasifikasikan sentimen:' tanpa menunjukkan contoh berlabel.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is prompt engineering?", question_id: "Apa itu prompt engineering?", option_a_en: "Designing effective prompts to get desired outputs from language models", option_a_id: "Merancang prompt efektif untuk mendapatkan output yang diinginkan dari model bahasa", option_b_en: "Engineering prompts", option_b_id: "Merekayasa prompt", option_c_en: "Building prompts", option_c_id: "Membangun prompt", option_d_en: "Coding prompts", option_d_id: "Mengkode prompt", correct_answer: "A", explanation_en: "Good prompts include clear instructions, examples, output format, and context for better results.", explanation_id: "Prompt yang baik mencakup instruksi jelas, contoh, format output, dan konteks untuk hasil lebih baik.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is chain-of-thought prompting?", question_id: "Apa itu chain-of-thought prompting?", option_a_en: "Prompting the model to show step-by-step reasoning", option_a_id: "Meminta model untuk menunjukkan penalaran langkah demi langkah", option_b_en: "Chaining prompts together", option_b_id: "Merantai prompt bersama", option_c_en: "Thinking in chains", option_c_id: "Berpikir dalam rantai", option_d_en: "Sequential questioning", option_d_id: "Pertanyaan berurutan", correct_answer: "A", explanation_en: "CoT prompting elicits reasoning steps, improving performance on math and logic problems.", explanation_id: "CoT prompting memunculkan langkah penalaran, meningkatkan performa pada masalah matematika dan logika.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is hallucination in LLMs?", question_id: "Apa itu halusinasi dalam LLM?", option_a_en: "Generating false or made-up information that sounds plausible", option_a_id: "Menghasilkan informasi palsu atau dibuat-buat yang terdengar masuk akal", option_b_en: "Seeing things", option_b_id: "Melihat sesuatu", option_c_en: "Model dreaming", option_c_id: "Model bermimpi", option_d_en: "Errors in output", option_d_id: "Error dalam output", correct_answer: "A", explanation_en: "LLMs may confidently state incorrect facts or cite non-existent sources. Major reliability concern.", explanation_id: "LLM mungkin dengan percaya diri menyatakan fakta yang salah atau mengutip sumber yang tidak ada. Kekhawatiran reliabilitas utama.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is RAG (Retrieval-Augmented Generation)?", question_id: "Apa itu RAG (Retrieval-Augmented Generation)?", option_a_en: "Combining retrieval of relevant documents with generation for grounded responses", option_a_id: "Menggabungkan retrieval dokumen relevan dengan generasi untuk respons yang membumi", option_b_en: "Random Answer Generation", option_b_id: "Random Answer Generation", option_c_en: "Rapid Answer Generator", option_c_id: "Rapid Answer Generator", option_d_en: "A type of cloth", option_d_id: "Jenis kain", correct_answer: "A", explanation_en: "RAG retrieves relevant passages from a knowledge base, providing context that reduces hallucination.", explanation_id: "RAG mengambil passage relevan dari basis pengetahuan, menyediakan konteks yang mengurangi halusinasi.", topic: "nlp", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is RLHF?", question_id: "Apa itu RLHF?", option_a_en: "Reinforcement Learning from Human Feedback - aligning LLMs with human preferences", option_a_id: "Reinforcement Learning from Human Feedback - menyelaraskan LLM dengan preferensi manusia", option_b_en: "Really Large Helpful Features", option_b_id: "Really Large Helpful Features", option_c_en: "Random Learning High Frequency", option_c_id: "Random Learning High Frequency", option_d_en: "Recursive Learning Helper Function", option_d_id: "Recursive Learning Helper Function", correct_answer: "A", explanation_en: "RLHF trains a reward model from human comparisons, then uses RL to optimize the LLM for that reward.", explanation_id: "RLHF melatih model reward dari perbandingan manusia, lalu menggunakan RL untuk mengoptimalkan LLM untuk reward itu.", topic: "nlp", difficulty: "hard", competition: "noai_prelim"},
  %{question_en: "What is instruction tuning?", question_id: "Apa itu instruction tuning?", option_a_en: "Fine-tuning on instruction-following examples to improve task generalization", option_a_id: "Fine-tuning pada contoh mengikuti instruksi untuk meningkatkan generalisasi tugas", option_b_en: "Tuning instructions", option_b_id: "Menyetel instruksi", option_c_en: "Writing instructions", option_c_id: "Menulis instruksi", option_d_en: "Following instructions", option_d_id: "Mengikuti instruksi", correct_answer: "A", explanation_en: "Instruction tuning trains on diverse tasks with instructions, making models better at following new instructions.", explanation_id: "Instruction tuning melatih pada tugas beragam dengan instruksi, membuat model lebih baik dalam mengikuti instruksi baru.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is the context window?", question_id: "Apa itu context window?", option_a_en: "The maximum number of tokens a model can process at once", option_a_id: "Jumlah maksimum token yang dapat diproses model sekaligus", option_b_en: "A window showing context", option_b_id: "Jendela yang menunjukkan konteks", option_c_en: "Text surrounding a word", option_c_id: "Teks di sekitar kata", option_d_en: "Training data size", option_d_id: "Ukuran data training", correct_answer: "A", explanation_en: "GPT-4 has 8k-128k tokens. Longer context allows more information but increases computation.", explanation_id: "GPT-4 memiliki 8k-128k token. Konteks lebih panjang memungkinkan lebih banyak informasi tapi meningkatkan komputasi.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is tokenizer vocabulary size?", question_id: "Apa itu ukuran kosakata tokenizer?", option_a_en: "The number of unique tokens the tokenizer can produce", option_a_id: "Jumlah token unik yang dapat dihasilkan tokenizer", option_b_en: "Number of words in dictionary", option_b_id: "Jumlah kata dalam kamus", option_c_en: "Model size", option_c_id: "Ukuran model", option_d_en: "Training data vocabulary", option_d_id: "Kosakata data training", correct_answer: "A", explanation_en: "Typical vocab sizes: 30k-100k tokens. Larger vocab = shorter sequences but bigger embedding matrix.", explanation_id: "Ukuran vocab tipikal: 30k-100k token. Vocab lebih besar = sequence lebih pendek tapi matriks embedding lebih besar.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is BLEU score?", question_id: "Apa itu skor BLEU?", option_a_en: "Bilingual Evaluation Understudy - metric for machine translation quality", option_a_id: "Bilingual Evaluation Understudy - metrik untuk kualitas machine translation", option_b_en: "Blue colored score", option_b_id: "Skor berwarna biru", option_c_en: "Basic Language Understanding Evaluation", option_c_id: "Basic Language Understanding Evaluation", option_d_en: "Balanced Language Evaluation Unit", option_d_id: "Balanced Language Evaluation Unit", correct_answer: "A", explanation_en: "BLEU measures n-gram overlap between generated and reference translations. Higher = better.", explanation_id: "BLEU mengukur overlap n-gram antara terjemahan yang dihasilkan dan referensi. Lebih tinggi = lebih baik.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is ROUGE score?", question_id: "Apa itu skor ROUGE?", option_a_en: "Recall-Oriented Understudy for Gisting Evaluation - metric for summarization", option_a_id: "Recall-Oriented Understudy for Gisting Evaluation - metrik untuk ringkasan", option_b_en: "Red colored score", option_b_id: "Skor berwarna merah", option_c_en: "Reading Oriented Understanding Gauge", option_c_id: "Reading Oriented Understanding Gauge", option_d_en: "Random Output Uniformity Gauge", option_d_id: "Random Output Uniformity Gauge", correct_answer: "A", explanation_en: "ROUGE measures overlap with reference summaries. ROUGE-1 (unigram), ROUGE-2 (bigram), ROUGE-L (LCS).", explanation_id: "ROUGE mengukur overlap dengan ringkasan referensi. ROUGE-1 (unigram), ROUGE-2 (bigram), ROUGE-L (LCS).", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is cosine similarity in NLP?", question_id: "Apa itu cosine similarity dalam NLP?", option_a_en: "Measuring semantic similarity between text embeddings using angle between vectors", option_a_id: "Mengukur kesamaan semantik antara embedding teks menggunakan sudut antara vektor", option_b_en: "Finding cosines in text", option_b_id: "Menemukan cosine dalam teks", option_c_en: "Trigonometric analysis", option_c_id: "Analisis trigonometri", option_d_en: "Distance measurement", option_d_id: "Pengukuran jarak", correct_answer: "A", explanation_en: "cos(A,B) = (A·B)/(|A||B|). Value of 1 means identical direction (similar meaning).", explanation_id: "cos(A,B) = (A·B)/(|A||B|). Nilai 1 berarti arah identik (makna serupa).", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is TF-IDF?", question_id: "Apa itu TF-IDF?", option_a_en: "Term Frequency-Inverse Document Frequency - weighting words by importance", option_a_id: "Term Frequency-Inverse Document Frequency - pembobotan kata berdasarkan kepentingan", option_b_en: "Text Filtering In Documents Format", option_b_id: "Text Filtering In Documents Format", option_c_en: "Total Frequency Index Data Format", option_c_id: "Total Frequency Index Data Format", option_d_en: "Token Frequency In Database", option_d_id: "Token Frequency In Database", correct_answer: "A", explanation_en: "TF-IDF upweights words frequent in a document but rare overall. Classic pre-neural IR technique.", explanation_id: "TF-IDF meningkatkan bobot kata yang sering dalam dokumen tapi jarang secara keseluruhan. Teknik IR pra-neural klasik.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is stopword removal?", question_id: "Apa itu penghapusan stopword?", option_a_en: "Removing common words like 'the', 'is', 'at' that carry little meaning", option_a_id: "Menghapus kata umum seperti 'the', 'is', 'at' yang membawa sedikit makna", option_b_en: "Stopping word processing", option_b_id: "Menghentikan pemrosesan kata", option_c_en: "Removing all words", option_c_id: "Menghapus semua kata", option_d_en: "Stopping the program", option_d_id: "Menghentikan program", correct_answer: "A", explanation_en: "Stopwords are filtered in traditional NLP to focus on content words. Less used in modern deep learning.", explanation_id: "Stopword difilter dalam NLP tradisional untuk fokus pada kata konten. Kurang digunakan dalam deep learning modern.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is lemmatization?", question_id: "Apa itu lemmatisasi?", option_a_en: "Reducing words to their base dictionary form (lemma)", option_a_id: "Mengurangi kata ke bentuk kamus dasarnya (lemma)", option_b_en: "Adding lemmas", option_b_id: "Menambahkan lemma", option_c_en: "Finding word roots", option_c_id: "Menemukan akar kata", option_d_en: "Word correction", option_d_id: "Koreksi kata", correct_answer: "A", explanation_en: "'running' → 'run', 'better' → 'good'. Uses morphological analysis, more accurate than stemming.", explanation_id: "'running' → 'run', 'better' → 'good'. Menggunakan analisis morfologis, lebih akurat dari stemming.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is stemming?", question_id: "Apa itu stemming?", option_a_en: "Cutting word endings to get root form (often crude)", option_a_id: "Memotong akhiran kata untuk mendapatkan bentuk akar (sering kasar)", option_b_en: "Growing stems", option_b_id: "Menumbuhkan batang", option_c_en: "Finding word stems", option_c_id: "Menemukan batang kata", option_d_en: "Plant analysis", option_d_id: "Analisis tanaman", correct_answer: "A", explanation_en: "Porter stemmer: 'running' → 'run', 'studies' → 'studi'. Fast but sometimes creates non-words.", explanation_id: "Porter stemmer: 'running' → 'run', 'studies' → 'studi'. Cepat tapi kadang membuat non-kata.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is part-of-speech (POS) tagging?", question_id: "Apa itu part-of-speech (POS) tagging?", option_a_en: "Labeling words with their grammatical category (noun, verb, adjective)", option_a_id: "Melabeli kata dengan kategori gramatikal mereka (kata benda, kata kerja, kata sifat)", option_b_en: "Tagging positions", option_b_id: "Menandai posisi", option_c_en: "Speech recognition", option_c_id: "Pengenalan suara", option_d_en: "Postal tagging", option_d_id: "Penandaan pos", correct_answer: "A", explanation_en: "POS tagging identifies word roles: 'The/DT quick/JJ fox/NN jumps/VBZ'. Used in parsing, NER.", explanation_id: "POS tagging mengidentifikasi peran kata: 'The/DT quick/JJ fox/NN jumps/VBZ'. Digunakan dalam parsing, NER.", topic: "nlp", difficulty: "easy", competition: "noai_prelim"},
  %{question_en: "What is dependency parsing?", question_id: "Apa itu dependency parsing?", option_a_en: "Analyzing grammatical relationships between words in a sentence", option_a_id: "Menganalisis hubungan gramatikal antara kata dalam kalimat", option_b_en: "Parsing dependencies in code", option_b_id: "Mem-parsing dependensi dalam kode", option_c_en: "Finding word pairs", option_c_id: "Menemukan pasangan kata", option_d_en: "Sentence counting", option_d_id: "Menghitung kalimat", correct_answer: "A", explanation_en: "Dependency parsing creates tree showing subject→verb, verb→object relationships.", explanation_id: "Dependency parsing membuat pohon yang menunjukkan hubungan subjek→kata kerja, kata kerja→objek.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is coreference resolution?", question_id: "Apa itu coreference resolution?", option_a_en: "Identifying when different expressions refer to the same entity", option_a_id: "Mengidentifikasi kapan ekspresi berbeda mengacu pada entitas yang sama", option_b_en: "Core reference finding", option_b_id: "Menemukan referensi inti", option_c_en: "Finding core words", option_c_id: "Menemukan kata inti", option_d_en: "Reference counting", option_d_id: "Menghitung referensi", correct_answer: "A", explanation_en: "'John said he would come' - 'he' refers to 'John'. Important for understanding context.", explanation_id: "'John said he would come' - 'he' mengacu pada 'John'. Penting untuk memahami konteks.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"},
  %{question_en: "What is word sense disambiguation?", question_id: "Apa itu word sense disambiguation?", option_a_en: "Determining which meaning of a word is used in context", option_a_id: "Menentukan makna mana dari kata yang digunakan dalam konteks", option_b_en: "Removing word meanings", option_b_id: "Menghapus makna kata", option_c_en: "Finding new meanings", option_c_id: "Menemukan makna baru", option_d_en: "Translating words", option_d_id: "Menerjemahkan kata", correct_answer: "A", explanation_en: "'Bank' can mean financial institution or river bank. WSD determines correct sense from context.", explanation_id: "'Bank' bisa berarti lembaga keuangan atau tepi sungai. WSD menentukan makna yang benar dari konteks.", topic: "nlp", difficulty: "medium", competition: "noai_prelim"}
]

IO.puts("Inserting NLP questions...")
count = NLPSeeder.insert_all(nlp_questions)
IO.puts("Inserted #{count} NLP questions")

total = Repo.aggregate(Question, :count)
IO.puts("\nTotal MCQ questions now: #{total}")
